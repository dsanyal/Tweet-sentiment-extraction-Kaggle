{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Extraction\n",
    "\n",
    "From Kaggle:\n",
    "\n",
    "\n",
    "\"My ridiculous dog is amazing.\" [sentiment: positive]\n",
    "\n",
    "With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tokenizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sample_sub = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.dtypes)\n",
    "train['textID'] = train['textID'].apply(str)\n",
    "train['text'] = train['text'].apply(str)\n",
    "train['selected_text'] = train['selected_text'].apply(str)\n",
    "train['sentiment'] = train['sentiment'].apply(str)\n",
    "\n",
    "test['textID'] = test['textID'].apply(str)\n",
    "test['text'] = test['text'].apply(str)\n",
    "test['sentiment'] = test['sentiment'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the task?\n",
    "\n",
    "- Given the tweet text and the sentiment of the tweet, the task at hand is to extract the text from the tweet that reflects the sentiment. How interesting!\n",
    "- One can formulate the problem as a **question-answering task**, where the sentiment is the *(short)* question, the tweet itself is the context and the selected text is the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  what interview! leave me alone\n",
      "Answer: leave me alone\n"
     ]
    }
   ],
   "source": [
    "ex = train.loc[3,:]\n",
    "context = ex.text\n",
    "answer = ex.selected_text\n",
    "print('Context: {}'.format(context))\n",
    "print('Answer: {}'.format(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      10005\n",
       "1        171\n",
       "2        116\n",
       "3         66\n",
       "27        57\n",
       "       ...  \n",
       "91         1\n",
       "59         1\n",
       "106        1\n",
       "90         1\n",
       "99         1\n",
       "Length: 98, dtype: int64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.sentiment == 'neutral',['text', 'selected_text']].apply(lambda row: len(row['text'].strip())-\n",
    "                                                                        len(row['selected_text'].strip()), axis=1 ).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and data preparation\n",
    "- The tokenization logic with offsets is inspired from Abhishek Thakur's Kaggle kernel [here]( https://www.kaggle.com/abhishek/roberta-inference-5-folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize the BERT base uncased tokenizer\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer('../input/BERT_uncased_vocab/vocab.txt', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_input_data(question, context, answer, tokenizer, max_len):\n",
    "    \n",
    "# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids   \n",
    "    input_tokens = enc.tokens\n",
    "    token_type_ids = enc.type_ids # These are the segment ids for differentiating question from context\n",
    "    attention_mask = enc.attention_mask\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    target_char_start  = context.find(answer)\n",
    "    target_char_end = target_char_start + len(answer) - 1\n",
    "    char_targets = [0]*len(context)\n",
    "    for i in range(target_char_start,target_char_end+1):\n",
    "        char_targets[i] = 1\n",
    "    \n",
    "    targets_index_context = []\n",
    "    \n",
    "    offsets_context = offsets[3:-1]\n",
    "    for ind, (i,j) in enumerate(offsets_context):\n",
    "        if sum(char_targets[i:j]) > 0:\n",
    "            targets_index_context.append(ind) \n",
    "           \n",
    "    target_start_ind = targets_index_context[0] \n",
    "    target_end_ind   = targets_index_context[-1]\n",
    "    \n",
    "    target_start_ind += 3\n",
    "    target_end_ind += 3\n",
    "    \n",
    "    # padding -- pad the vectors if their lengths exceed max_len, else truncate at max_len\n",
    "    pad_len = max_len - len(token_type_ids)\n",
    "    if(pad_len> 0):\n",
    "        token_type_ids = token_type_ids + [0]*pad_len\n",
    "        input_ids = input_ids + [0]*pad_len\n",
    "        attention_mask = attention_mask + [0]*pad_len\n",
    "        offsets = offsets + [(0,0)]*pad_len    \n",
    "    else:\n",
    "        input_ids = input_ids[:max_len-1] + [102]\n",
    "\n",
    "    \n",
    "    output_dict = {'token_type_ids': token_type_ids,\n",
    "                  'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'target_start': target_start_ind,\n",
    "                  'target_end': target_end_ind,\n",
    "                  'input_tokens': input_tokens,\n",
    "                   'offsets': offsets,\n",
    "                   'attention_mask': attention_mask,\n",
    "                   'sentiment': question,\n",
    "                   'context': context,\n",
    "                   'answer': answer\n",
    "                  }\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  negative\n",
      "Tweet:   what interview! leave me alone\n",
      "Selected text:  leave me alone\n",
      "Input tokens:  ['[CLS]', 'negative', '[SEP]', 'what', 'interview', '!', 'leave', 'me', 'alone', '[SEP]']\n",
      "Token type ids:  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input_ids:  [101, 4997, 102, 2054, 4357, 999, 2681, 2033, 2894, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Target start index: 6, Target end index: 8\n",
      "Attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Offsets:  [(0, 0), (0, 8), (0, 0), (1, 5), (6, 15), (15, 16), (17, 22), (23, 25), (26, 31), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "data_example = preprocess_input_data(train.sentiment[3], train.text[3], train.selected_text[3], tokenizer, max_len = 20)\n",
    "print('Sentiment: ',data_example['sentiment'])\n",
    "print('Tweet: ',data_example['context'])\n",
    "print('Selected text: ',data_example['answer'])\n",
    "print('Input tokens: ',data_example['input_tokens'])\n",
    "print('Token type ids: ', data_example['token_type_ids'])\n",
    "print('Input_ids: ',data_example['input_ids'])\n",
    "print('Target start index: {}, Target end index: {}'.format(data_example['target_start'], data_example['target_end']))\n",
    "print('Attention mask: ', data_example['attention_mask'])\n",
    "print('Offsets: ',data_example['offsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tokens = []\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    answer = train.selected_text[i]\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids  \n",
    "    len_tokens.append(len(input_ids))\n",
    "\n",
    "print(max(len_tokens))\n",
    "plt.plot(np.arange(len(len_tokens)), len_tokens, 'bo');\n",
    "plt.ylabel('Length of encoded tokens (+special tokens)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the training set\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "input_ids = np.zeros((train.shape[0],max_len))\n",
    "token_type_ids = np.zeros((train.shape[0],max_len))\n",
    "attention_mask = np.zeros((train.shape[0],max_len))\n",
    "start_ids = np.zeros((train.shape[0],max_len))\n",
    "end_ids = np.zeros((train.shape[0],max_len))\n",
    "\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    answer = train.selected_text[i]\n",
    "    processed_data = preprocess_input_data(question, context, answer, tokenizer, max_len = max_len)\n",
    "    input_ids[i,:] = processed_data['input_ids']\n",
    "    token_type_ids[i,:] = processed_data['token_type_ids']\n",
    "    attention_mask[i,:] = processed_data['attention_mask']\n",
    "    start_ids[i,:] = to_categorical(processed_data['target_start'], num_classes = max_len)\n",
    "    end_ids[i,:] = to_categorical(processed_data['target_end'], num_classes = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the test set\n",
    "def preprocess_test_data(question, context, tokenizer, max_len):\n",
    "    \n",
    "# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids   \n",
    "    input_tokens = enc.tokens\n",
    "    token_type_ids = enc.type_ids # These are the segment ids for differentiating question from context\n",
    "    attention_mask = enc.attention_mask\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    \n",
    "    # padding -- pad the vectors if their lengths exceed max_len, else truncate at max_len\n",
    "    pad_len = max_len - len(token_type_ids)\n",
    "    if(pad_len> 0):\n",
    "        token_type_ids = token_type_ids + [0]*pad_len\n",
    "        input_ids = input_ids + [0]*pad_len\n",
    "        attention_mask = attention_mask + [0]*pad_len\n",
    "        offsets = offsets + [(0,0)]*pad_len    \n",
    "    else:\n",
    "        token_type_ids = token_type_ids[:max_len]\n",
    "        input_ids = input_ids[:max_len]\n",
    "        attention_mask = attention_mask[:max_len]\n",
    "        offsets = offsets[:max_len]\n",
    "\n",
    "    \n",
    "    output_dict = {'token_type_ids': token_type_ids,\n",
    "                  'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'input_tokens': input_tokens,\n",
    "                   'offsets': offsets,\n",
    "                   'attention_mask': attention_mask,\n",
    "                   'sentiment': question,\n",
    "                   'context': context,\n",
    "                  }\n",
    "    return output_dict\n",
    "\n",
    "input_ids_test = np.zeros((test.shape[0],max_len))\n",
    "token_type_ids_test = np.zeros((test.shape[0],max_len))\n",
    "attention_mask_test = np.zeros((test.shape[0],max_len))\n",
    "\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    question = test.sentiment[i]\n",
    "    context= test.text[i]\n",
    "    processed_data_test = preprocess_test_data(question, context, tokenizer, max_len = max_len)\n",
    "    input_ids_test[i,:] = processed_data_test['input_ids']\n",
    "    token_type_ids_test[i,:] = processed_data_test['token_type_ids']\n",
    "    attention_mask_test[i,:] = processed_data_test['attention_mask']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model\n",
    "- The BERT output (768 dimensional vector for each token) is fed in to a CNN layer with ReLU acitvation before passing on to a linear layer with softmax activation. The output of this head represents the  probability of a token being the start index of the answer. Similarly, we construct another head for the end index.\n",
    "- Dropout is added right after BERT embedding layer for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    config = transformers.BertConfig()\n",
    "\n",
    "def build_model():\n",
    "    inp_ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    inp_tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    inp_att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    \n",
    "    bert_tf = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    x = bert_tf({'input_ids': inp_ids, 'token_type_ids': inp_tok, 'attention_mask': inp_att})[0]\n",
    "\n",
    " # Refer to https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15812785.pdf   \n",
    "\n",
    "\n",
    "#    print(x.shape)\n",
    "    h1 = tf.keras.layers.Dropout(0.1)(x) \n",
    "    h1 = tf.keras.layers.Conv1D(128, 2,padding='same')(h1)\n",
    "    h1 = tf.keras.layers.BatchNormalization()(h1)\n",
    "    h1 = tf.keras.layers.ReLU()(h1)\n",
    "    h1 = tf.keras.layers.Dense(1)(h1)\n",
    "#    print(h1.shape)\n",
    "    h1 = tf.keras.layers.Flatten()(h1)\n",
    "#    print(h1.shape)\n",
    "    h1 = tf.keras.layers.Activation('softmax')(h1)\n",
    "#    print(h1.shape)\n",
    "\n",
    "    h2 = tf.keras.layers.Dropout(0.1)(x) \n",
    "    h2 = tf.keras.layers.Conv1D(128, 2,padding='same')(h2)\n",
    "    h2 = tf.keras.layers.BatchNormalization()(h2)\n",
    "    h2 = tf.keras.layers.ReLU()(h2)\n",
    "    h2 = tf.keras.layers.Dense(1)(h2)\n",
    "    h2 = tf.keras.layers.Flatten()(h2)\n",
    "    h2 = tf.keras.layers.Activation('softmax')(h2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inp_ids, inp_att, inp_tok], outputs=[h1,h2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "    model.compile(optimizer = optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_4 (TFBertModel)   ((None, 128, 768), ( 109482240   input_9[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_185 (Dropout)           (None, 128, 768)     0           tf_bert_model_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_186 (Dropout)           (None, 128, 768)     0           tf_bert_model_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 128, 128)     196736      dropout_185[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 128)     196736      dropout_186[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128)     512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128)     512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 128, 128)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 128, 128)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128, 1)       129         re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 1)       129         re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,876,994\n",
      "Trainable params: 109,876,482\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5890913411191024"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline jaccard score\n",
    "train.apply(lambda row: jaccard(row['text'], row['selected_text']), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_ids, attention_mask, token_type_ids, input_ids_test,attention_mask_test,token_type_ids_test,kfold_n_splits = 5, epochs=3):\n",
    "    jaccard_scores = []\n",
    "    oof_start = np.zeros((input_ids.shape[0],max_len))\n",
    "    oof_end = np.zeros((input_ids.shape[0],max_len))\n",
    "    preds_start = np.zeros((input_ids_test.shape[0],max_len))\n",
    "    preds_end = np.zeros((input_ids_test.shape[0],max_len))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=kfold_n_splits,shuffle=True,random_state=9999)\n",
    "    for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "        print('#'*25)\n",
    "        print('### FOLD {}'.format(fold+1))\n",
    "        print('#'*25)\n",
    "        K.clear_session()\n",
    "        model = build_model()\n",
    "        \n",
    "        sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "            'bert-fold{}.h5'.format(fold+1), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "            save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "        model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]],\n",
    "              [start_ids[idxT,], end_ids[idxT,]], \n",
    "              epochs=epochs, batch_size=16, verbose=1, callbacks=[sv],\n",
    "              validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "                               [start_ids[idxV,], end_ids[idxV,]]))\n",
    "    \n",
    "#    print('Loading model...')\n",
    "#    model.load_weights('bert-fold%i.h5'%(fold))\n",
    "        shutil.copy2(r'bert-fold{}.h5'.format(fold+1), r'/content/gdrive/My Drive/')\n",
    "    \n",
    "        print('Predicting out-of-fold answer span')\n",
    "        oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=1)\n",
    "      \n",
    "    # DISPLAY FOLD JACCARD\n",
    "        jac_oof = []\n",
    "        for k in idxV:\n",
    "            a = np.argmax(oof_start[k,])\n",
    "            b = np.argmax(oof_end[k,])\n",
    "            true_answer = train.loc[k,'selected_text']\n",
    "            if (train.loc[k, 'sentiment'] == 'neutral'):\n",
    "                  pred_answer = train.loc[k, 'text']\n",
    "            elif a>b: \n",
    "                encoding = tokenizer.encode(train.loc[k, 'sentiment'], train.loc[k,'text'])\n",
    "                pred_answer = tokenizer.decode(encoding.ids[a:-1]) \n",
    "            else:\n",
    "                encoding = tokenizer.encode(train.loc[k, 'sentiment'], train.loc[k,'text'])\n",
    "                pred_answer = tokenizer.decode(encoding.ids[a:b+1])        \n",
    "            jac_oof.append(jaccard(pred_answer, true_answer))\n",
    "        jaccard_scores.append(np.mean(jac_oof))\n",
    "        print('FOLD %i Jaccard ='%(fold+1),np.mean(jac_oof))\n",
    "\n",
    "        print('Predicting Test answer span')\n",
    "        preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test])\n",
    "        preds_start += preds[0]/kfold_n_splits\n",
    "        preds_end += preds[1]/kfold_n_splits\n",
    "    return {'pred_test_start': preds_start, 'pred_test_end': preds_end,\n",
    "          'jaccard_kfold': jaccard_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = train_model(input_ids, attention_mask, token_type_ids, input_ids_test,attention_mask_test,token_type_ids_test,kfold_n_splits = 5, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_start, preds_end = out['pred_test_start'], out['pred_test_end']\n",
    "for i in range(input_ids_test.shape[0]):\n",
    "    a = np.argmax(preds_start[i,])\n",
    "    b = np.argmax(preds_end[i,])\n",
    "    if (a>b or test.loc[i, 'sentiment'] == 'neutral'): # if start index comes after the end index\n",
    "        pred_answer = test.loc[i, 'text']\n",
    "        test.loc[i,'selected_text'] = pred_answer\n",
    "    else:\n",
    "        encoding = tokenizer.encode(test.loc[i, 'sentiment'], test.loc[i,'text'])\n",
    "        pred_answer = tokenizer.decode(encoding.ids[a:b+1])  \n",
    "        test.loc[i,'selected_text'] = pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['textID', 'selected_text']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "#####BPE tokenizer#####\n",
      "[20760, 232, 328]\n",
      " hello world!\n",
      "#####RoBERTa tokenizer#####\n",
      "[20760, 232, 328]\n",
      " hello world!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=ROBERTA_PATH+'vocab-roberta-base.json', \n",
    "                                             merges_file=ROBERTA_PATH+'merges-roberta-base.txt', \n",
    "                                             lowercase=True,add_prefix_space=True)\n",
    "text = \"Hello world!\"\n",
    "print(text)\n",
    "print('#'*5+'BPE tokenizer'+'#'*5)\n",
    "enc = tokenizer.encode(text) \n",
    "print(enc.ids)\n",
    "print(tokenizer.decode(enc.ids))\n",
    "\n",
    "print('#'*5+'RoBERTa tokenizer'+'#'*5)\n",
    "tokenizer_roberta = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "text = \" \" + \" \".join(text.lower().split())\n",
    "#text = text.lower()\n",
    "encode_roberta = tokenizer_roberta.encode_plus(text, add_special_tokens=False)\n",
    "print(encode_roberta['input_ids'])\n",
    "#print(tokenizer_roberta.convert_ids_to_tokens(encode_roberta['input_ids']))\n",
    "print(tokenizer_roberta.decode(encode_roberta['input_ids'],clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_PATH = '../input/tf-roberta/'\n",
    "# Intialize the RoBERTa base tokenizer\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file = ROBERTA_PATH+'vocab-roberta-base.json', \n",
    "                                             merges_file = ROBERTA_PATH+'merges-roberta-base.txt',\n",
    "                                            lowercase = True,add_prefix_space=True)\n",
    "\n",
    "def preprocess_input_data_roberta(question, context, answer, tokenizer, max_len):\n",
    "    \n",
    "# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n",
    "    context = \" \" + \" \".join(context.split())\n",
    "    answer = \" \" + \" \".join(answer.split())\n",
    "    enc_question = tokenizer.encode(question)\n",
    "    enc_context = tokenizer.encode(context)\n",
    "    input_ids = [0] + enc_question.ids + [2,2] +   enc_context.ids + [2]\n",
    "    input_tokens = enc_question.tokens + enc_context.tokens\n",
    "    token_type_ids = [0]* len(input_ids) # not relevant for RoBERTa\n",
    "    attention_mask = [1]* len(input_ids)\n",
    "    \n",
    "    offsets_question = enc_question.offsets\n",
    "    offsets_context = enc_context.offsets\n",
    "    offsets = [(0,0)] + offsets_question + [(0,0)]*2 + offsets_context +[(0,0)]\n",
    "    \n",
    "    target_char_start  = context.find(answer)\n",
    "    target_char_end = target_char_start + len(answer) - 1\n",
    "    char_targets = [0]*len(context)\n",
    "    for i in range(target_char_start,target_char_end+1):\n",
    "        char_targets[i] = 1\n",
    "    targets_index_context = []    \n",
    "    for ind, (i,j) in enumerate(offsets_context):\n",
    "        if sum(char_targets[i:j]) > 0:\n",
    "            targets_index_context.append(ind) \n",
    "           \n",
    "    target_start_ind = targets_index_context[0] + 4 \n",
    "    target_end_ind   = targets_index_context[-1] + 4\n",
    "    \n",
    "\n",
    "        # padding -- pad the vectors if their lengths exceed max_len\n",
    "    pad_len = max_len - len(token_type_ids)\n",
    "    if(pad_len> 0):\n",
    "        token_type_ids = token_type_ids + [0]*pad_len\n",
    "        input_ids = input_ids + [1]*pad_len    # [1] is the <pad> token in RoBERTa\n",
    "        attention_mask = attention_mask + [0]*pad_len\n",
    "        offsets = offsets + [(0,0)]*pad_len    \n",
    "\n",
    "        \n",
    "    output_dict = {'token_type_ids': token_type_ids,\n",
    "                  'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'target_start': target_start_ind,\n",
    "                  'target_end': target_end_ind,\n",
    "                  'input_tokens': input_tokens,\n",
    "                   'offsets': offsets,\n",
    "                   'attention_mask': attention_mask,\n",
    "                   'sentiment': question,\n",
    "                   'context': context,\n",
    "                   'answer': answer\n",
    "                  }\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:negative\n",
      "Tweet: what interview! leave me alone\n",
      "Selected text: leave me alone\n",
      "Input tokens: ['Ġnegative', 'Ġwhat', 'Ġinterview', '!', 'Ġleave', 'Ġme', 'Ġalone']\n",
      "Token type ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input_ids: [0, 2430, 2, 2, 99, 1194, 328, 989, 162, 1937, 2, 1, 1, 1, 1]\n",
      "Target start index: 7, Target end index: 9\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "offsets: [(0, 0), (0, 8), (0, 0), (0, 0), (0, 5), (5, 15), (15, 16), (16, 22), (22, 25), (25, 31), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "data_example = preprocess_input_data_roberta(train.sentiment[3], train.text[3], train.selected_text[3], tokenizer, max_len = 15)\n",
    "print('Sentiment:{}'.format(data_example['sentiment']))\n",
    "print('Tweet:{}'.format(data_example['context']))\n",
    "print('Selected text:{}'.format(data_example['answer']))\n",
    "print('Input tokens: {}'.format(data_example['input_tokens']))\n",
    "print('Token type ids: {}'.format(data_example['token_type_ids']))\n",
    "print('Input_ids: {}'.format(data_example['input_ids']))\n",
    "print('Target start index: {}, Target end index: {}'.format(data_example['target_start'], data_example['target_end']))\n",
    "print('Attention mask: {}'.format(data_example['attention_mask']))\n",
    "print('offsets: {}'.format(data_example['offsets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip the starting blankspace at front of some tweets\n",
    "\n",
    "#train.loc[:, 'text'] = train.loc[:, 'text'].apply(lambda tweet: tweet.strip())\n",
    "#train.loc[:, 'selected_text'] = train.loc[:, 'selected_text'].apply(lambda selected_text: selected_text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode the training set\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "input_ids = np.ones((train.shape[0],max_len), dtype = np.int64)\n",
    "token_type_ids = np.zeros((train.shape[0],max_len), dtype = np.int8)\n",
    "attention_mask = np.zeros((train.shape[0],max_len), dtype = np.int8)\n",
    "start_ids = np.zeros((train.shape[0],max_len), dtype = np.int64)\n",
    "end_ids = np.zeros((train.shape[0],max_len), dtype = np.int64)\n",
    "\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    answer = train.selected_text[i]\n",
    "    processed_data = preprocess_input_data_roberta(question, context, answer, tokenizer, max_len = max_len)\n",
    "    input_ids[i,:] = processed_data['input_ids']\n",
    "    token_type_ids[i,:] = processed_data['token_type_ids']\n",
    "    attention_mask[i,:] = processed_data['attention_mask']\n",
    "    start_ids[i,:] = to_categorical(processed_data['target_start'], num_classes = max_len)\n",
    "    end_ids[i,:] = to_categorical(processed_data['target_end'], num_classes = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_abhishek(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:negative\n",
      "Tweet: what interview! leave me alone\n",
      "Selected text: leave me alone\n",
      "Token type ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Input_ids: [0, 2430, 2, 2, 101, 2054, 4357, 999, 2681, 2033, 2894, 102, 2, 1, 1]\n",
      "Target start index: 8, Target end index: 10\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "offsets: [(0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (1, 5), (6, 15), (15, 16), (17, 22), (23, 25), (26, 31), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "k=3\n",
    "out = process_data_abhishek(train.text[k], train.selected_text[k], train.sentiment[k], tokenizer, max_len=15)\n",
    "print('Sentiment:{}'.format(out['sentiment']))\n",
    "print('Tweet:{}'.format(out['orig_tweet']))\n",
    "print('Selected text:{}'.format(out['orig_selected']))\n",
    "print('Token type ids: {}'.format(out['token_type_ids']))\n",
    "print('Input_ids: {}'.format(out['ids']))\n",
    "print('Target start index: {}, Target end index: {}'.format(out['targets_start'], out['targets_end']))\n",
    "print('Attention mask: {}'.format(out['mask']))\n",
    "print('offsets: {}'.format(out['offsets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leave'"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([out['ids'][out['targets_start']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids_a = np.ones((train.shape[0],max_len), dtype = np.int64)\n",
    "token_type_ids_a = np.zeros((train.shape[0],max_len), dtype = np.int8)\n",
    "attention_mask_a = np.zeros((train.shape[0],max_len), dtype = np.int8)\n",
    "start_ids_a = np.zeros((train.shape[0],max_len), dtype = np.int64)\n",
    "end_ids_a = np.zeros((train.shape[0],max_len), dtype = np.int64)\n",
    "\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    answer = train.selected_text[i]\n",
    "    processed_data = process_data_abhishek(context, answer, question,tokenizer, max_len = max_len)\n",
    "    input_ids_a[i,:] = processed_data['ids']\n",
    "    token_type_ids_a[i,:] = processed_data['token_type_ids']\n",
    "    attention_mask_a[i,:] = processed_data['mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input_ids_a == input_ids).sum(axis=1)==128).sum() == input_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example tokenization: RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'Hello world, excited to be here!'\n",
    "question = 'positive'\n",
    "answer = 'excited to be'\n",
    "\n",
    "context = \" \" + \" \".join(context.split()) \n",
    "answer = \" \" + \" \".join(answer.split())\n",
    "\n",
    "enc_question = tokenizer.encode(question)\n",
    "enc_context = tokenizer.encode(context)\n",
    "input_ids = [0] + enc_question.ids + [2,2] +   enc_context.ids + [2]\n",
    "input_tokens = enc_question.tokens + enc_context.tokens\n",
    "token_type_ids = [0]* len(input_ids) # not relevant for RoBERTa\n",
    "attention_mask = [1]* len(input_ids)\n",
    "\n",
    "offsets_question = enc_question.offsets\n",
    "offsets_context = enc_context.offsets\n",
    "offsets = [(0,0)] + offsets_question + [(0,0)]*2 + offsets_context +[(0,0)]\n",
    "\n",
    "target_char_start  = context.find(answer)\n",
    "target_char_end = target_char_start + len(answer) - 1\n",
    "char_targets = [0]*len(context)\n",
    "for i in range(target_char_start,target_char_end+1):\n",
    "    char_targets[i] = 1\n",
    "targets_index_context = []    \n",
    "for ind, (i,j) in enumerate(offsets_context):\n",
    "    if sum(char_targets[i:j]) > 0:\n",
    "        targets_index_context.append(ind) \n",
    "\n",
    "target_start_ind = targets_index_context[0] + 4 \n",
    "target_end_ind   = targets_index_context[-1] + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (6, 12), (12, 13), (13, 21), (21, 24), (24, 27), (27, 32), (32, 33)]"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġhello', 'Ġworld', ',', 'Ġexcited', 'Ġto', 'Ġbe', 'Ġhere', '!']"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_context.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' excited to be'"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ids = [0] + enc_question.ids + [2,2] +   enc_context.ids + [2]\n",
    "tokenizer.decode(enc_ids[target_start_ind:target_end_ind+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' excited to be'"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
