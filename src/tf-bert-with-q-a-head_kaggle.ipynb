{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Extraction\n",
    "\n",
    "From Kaggle:\n",
    "\n",
    "\n",
    "\"My ridiculous dog is amazing.\" [sentiment: positive]\n",
    "\n",
    "With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-tf_model.h5\n",
      "/kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tokenizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical  \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "sample_sub = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.dtypes)\n",
    "train['textID'] = train['textID'].apply(str)\n",
    "train['text'] = train['text'].apply(str)\n",
    "train['selected_text'] = train['selected_text'].apply(str)\n",
    "train['sentiment'] = train['sentiment'].apply(str)\n",
    "\n",
    "test['textID'] = test['textID'].apply(str)\n",
    "test['text'] = test['text'].apply(str)\n",
    "test['sentiment'] = test['sentiment'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the task?\n",
    "\n",
    "- Given the tweet text and the sentiment of the tweet, the task at hand is to extract the text from the tweet that reflects the sentiment.\n",
    "- One can formulate the problem as a question-answering task, where the sentiment is the *(short)* question, the tweet itself is the context and the selected text is the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what interview! leave me alone\n",
      "leave me alone\n"
     ]
    }
   ],
   "source": [
    "ex = train.loc[3,:]\n",
    "context = ex.text\n",
    "answer = ex.selected_text\n",
    "print(context)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice the extra space at the beginning of the context string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the sentiments are neutral!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     10005\n",
       "1       171\n",
       "2       116\n",
       "3        66\n",
       "27       57\n",
       "26       39\n",
       "5        38\n",
       "8        37\n",
       "4        37\n",
       "6        35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.sentiment == 'neutral',['text', 'selected_text']].apply(lambda row: len(row['text'].strip())-\n",
    "                                                                        len(row['selected_text'].strip()), axis=1 ).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What you notice above is the fact that for most of the neutral sentiments, the selected text (answer) is the same as the text/tweet (context). This is in line with what you'd expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\n",
    "# Intialize the BERT base uncased tokenizer\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "\n",
    "def preprocess_input_data(question, context, answer, tokenizer, max_len):\n",
    "    \n",
    "# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids   \n",
    "    input_tokens = enc.tokens\n",
    "    token_type_ids = enc.type_ids # These are the segment ids for differentiating question from context\n",
    "    attention_mask = enc.attention_mask\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    target_char_start  = context.find(answer)\n",
    "    target_char_end = target_char_start + len(answer) - 1\n",
    "    char_targets = [0]*len(context)\n",
    "    for i in range(target_char_start,target_char_end+1):\n",
    "        char_targets[i] = 1\n",
    "    \n",
    "    targets_index_context = []\n",
    "    \n",
    "    offsets_context = offsets[3:-1]\n",
    "    for ind, (i,j) in enumerate(offsets_context):\n",
    "        if sum(char_targets[i:j]) > 0:\n",
    "            targets_index_context.append(ind) \n",
    "           \n",
    "    target_start_ind = targets_index_context[0] \n",
    "    target_end_ind   = targets_index_context[-1]\n",
    "    \n",
    "    target_start_ind += 3\n",
    "    target_end_ind += 3\n",
    "    \n",
    "    # padding -- pad the vectors if their lengths exceed max_len, else truncate at max_len\n",
    "    pad_len = max_len - len(token_type_ids)\n",
    "    if(pad_len> 0):\n",
    "        token_type_ids = token_type_ids + [0]*pad_len\n",
    "        input_ids = input_ids + [0]*pad_len\n",
    "        attention_mask = attention_mask + [0]*pad_len\n",
    "        offsets = offsets + [(0,0)]*pad_len    \n",
    "    else:\n",
    "        token_type_ids = token_type_ids[:max_len]\n",
    "        input_ids = input_ids[:max_len]\n",
    "        attention_mask = attention_mask[:max_len]\n",
    "        offsets = offsets[:max_len]\n",
    "\n",
    "    \n",
    "    output_dict = {'token_type_ids': token_type_ids,\n",
    "                  'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'target_start': target_start_ind,\n",
    "                  'target_end': target_end_ind,\n",
    "                  'input_tokens': input_tokens,\n",
    "                   'offsets': offsets,\n",
    "                   'attention_mask': attention_mask,\n",
    "                   'sentiment': question,\n",
    "                   'context': context,\n",
    "                   'answer': answer\n",
    "                  }\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  negative\n",
      "Tweet:   what interview! leave me alone\n",
      "Selected text:  leave me alone\n",
      "Input tokens:  ['[CLS]', 'negative', '[SEP]', 'what', 'interview', '!', 'leave', 'me', 'alone', '[SEP]']\n",
      "Token type ids:  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "Input_ids:  [101, 4997, 102, 2054, 4357, 999, 2681, 2033, 2894, 102, 0, 0, 0, 0, 0]\n",
      "Target start index: 6, Target end index: 8\n",
      "Attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "Offsets:  [(0, 0), (0, 8), (0, 0), (1, 5), (6, 15), (15, 16), (17, 22), (23, 25), (26, 31), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "data_example = preprocess_input_data(train.sentiment[3], train.text[3], train.selected_text[3], tokenizer, max_len = 15)\n",
    "print('Sentiment: ',data_example['sentiment'])\n",
    "print('Tweet: ',data_example['context'])\n",
    "print('Selected text: ',data_example['answer'])\n",
    "print('Input tokens: ',data_example['input_tokens'])\n",
    "print('Token type ids: ', data_example['token_type_ids'])\n",
    "print('Input_ids: ',data_example['input_ids'])\n",
    "print('Target start index: {}, Target end index: {}'.format(data_example['target_start'], data_example['target_end']))\n",
    "print('Attention mask: ', data_example['attention_mask'])\n",
    "print('Offsets: ',data_example['offsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_tokens = []\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids  \n",
    "    len_tokens.append(len(input_ids))\n",
    "\n",
    "print(max(len_tokens))\n",
    "plt.plot(np.arange(len(len_tokens)), len_tokens, 'bo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "\n",
    "\n",
    "input_ids = np.zeros((train.shape[0],max_len))\n",
    "token_type_ids = np.zeros((train.shape[0],max_len))\n",
    "attention_mask = np.zeros((train.shape[0],max_len))\n",
    "start_ids = np.zeros((train.shape[0],max_len))\n",
    "end_ids = np.zeros((train.shape[0],max_len))\n",
    "\n",
    "\n",
    "for i in range(train.shape[0]):\n",
    "    question = train.sentiment[i]\n",
    "    context= train.text[i]\n",
    "    answer = train.selected_text[i]\n",
    "    processed_data = preprocess_input_data(question, context, answer, tokenizer, max_len = max_len)\n",
    "    input_ids[i,:] = processed_data['input_ids']\n",
    "    token_type_ids[i,:] = processed_data['token_type_ids']\n",
    "    attention_mask[i,:] = processed_data['attention_mask']\n",
    "    start_ids[i,:] = to_categorical(processed_data['target_start'], num_classes = max_len)\n",
    "    end_ids[i,:] = to_categorical(processed_data['target_end'], num_classes = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_test_data(question, context, tokenizer, max_len):\n",
    "    \n",
    "# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n",
    "    enc = tokenizer.encode(question,context)\n",
    "    input_ids = enc.ids   \n",
    "    input_tokens = enc.tokens\n",
    "    token_type_ids = enc.type_ids # These are the segment ids for differentiating question from context\n",
    "    attention_mask = enc.attention_mask\n",
    "    offsets = enc.offsets\n",
    "    \n",
    "    \n",
    "    # padding -- pad the vectors if their lengths exceed max_len, else truncate at max_len\n",
    "    pad_len = max_len - len(token_type_ids)\n",
    "    if(pad_len> 0):\n",
    "        token_type_ids = token_type_ids + [0]*pad_len\n",
    "        input_ids = input_ids + [0]*pad_len\n",
    "        attention_mask = attention_mask + [0]*pad_len\n",
    "        offsets = offsets + [(0,0)]*pad_len    \n",
    "    else:\n",
    "        token_type_ids = token_type_ids[:max_len]\n",
    "        input_ids = input_ids[:max_len]\n",
    "        attention_mask = attention_mask[:max_len]\n",
    "        offsets = offsets[:max_len]\n",
    "\n",
    "    \n",
    "    output_dict = {'token_type_ids': token_type_ids,\n",
    "                  'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'input_tokens': input_tokens,\n",
    "                   'offsets': offsets,\n",
    "                   'attention_mask': attention_mask,\n",
    "                   'sentiment': question,\n",
    "                   'context': context,\n",
    "                  }\n",
    "    return output_dict\n",
    "\n",
    "input_ids_test = np.zeros((test.shape[0],max_len))\n",
    "token_type_ids_test = np.zeros((test.shape[0],max_len))\n",
    "attention_mask_test = np.zeros((test.shape[0],max_len))\n",
    "\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    question = test.sentiment[i]\n",
    "    context= test.text[i]\n",
    "    processed_data_test = preprocess_test_data(question, context, tokenizer, max_len = max_len)\n",
    "    input_ids_test[i,:] = processed_data_test['input_ids']\n",
    "    token_type_ids_test[i,:] = processed_data_test['token_type_ids']\n",
    "    attention_mask_test[i,:] = processed_data_test['attention_mask']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model\n",
    "- The BERT output (768 dimensional vector for each token) is passed on to a CNN layer followed by a linear layer with softmax activation. The linear layer equivalent to a 1-convolution in the context of NLP (or 1x1 convolution in vision). The output of this head represents the  probability of a token being the start index of the answer. Similarly, we construct another head for the end index.\n",
    "- Dropout is added right after BERT embedding layer for regularization.\n",
    "- The model is inspired by Chris Deotte's awesome kernel [here](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    config = transformers.BertConfig()\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n",
    "    \n",
    "    config = transformers.BertConfig()\n",
    "    bert_tf = transformers.TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    x = bert_tf({'input_ids': ids, 'token_type_ids': tok, 'attention_mask': att})[0]\n",
    "\n",
    " # Refer to https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15812785.pdf   \n",
    "\n",
    "#    print(x.shape)\n",
    "    \n",
    "    h1 = tf.keras.layers.Dropout(0.1)(x) \n",
    "    h1 = tf.keras.layers.Conv1D(128, 2,padding='same')(h1)\n",
    "    h1 = tf.keras.layers.BatchNormalization()(h1)\n",
    "    h1 = tf.keras.layers.ReLU()(h1)\n",
    "    h1 = tf.keras.layers.Dense(1)(h1)\n",
    "    h1 = tf.keras.layers.Flatten()(h1)\n",
    "    h1 = tf.keras.layers.Activation('softmax')(h1)\n",
    "\n",
    "    h2 = tf.keras.layers.Dropout(0.1)(x) \n",
    "    h2 = tf.keras.layers.Conv1D(128, 2,padding='same')(h2)\n",
    "    h2 = tf.keras.layers.BatchNormalization()(h2)\n",
    "    h2 = tf.keras.layers.ReLU()(h2)\n",
    "    h2 = tf.keras.layers.Dense(1)(h2)\n",
    "    h2 = tf.keras.layers.Flatten()(h2)\n",
    "    h2 = tf.keras.layers.Activation('softmax')(h2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[h1,h2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=4e-5)\n",
    "    model.compile(optimizer = optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_3[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 128, 768)     0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 128, 768)     0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 128, 128)     196736      dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 128)     196736      dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128)     512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128)     512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 128, 128)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 128, 128)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128, 1)       129         re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 1)       129         re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 128)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,876,994\n",
      "Trainable params: 109,876,482\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "# Let's take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_ids, attention_mask, token_type_ids, input_ids_test,attention_mask_test,token_type_ids_test,kfold_n_splits = 5, epochs=3):\n",
    "  jaccard_scores = []\n",
    "  oof_start = np.zeros((input_ids.shape[0],max_len))\n",
    "  oof_end = np.zeros((input_ids.shape[0],max_len))\n",
    "  preds_start = np.zeros((input_ids_test.shape[0],max_len))\n",
    "  preds_end = np.zeros((input_ids_test.shape[0],max_len))\n",
    "\n",
    "  skf = StratifiedKFold(n_splits=kfold_n_splits,shuffle=True,random_state=1212)\n",
    "  for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD {}'.format(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "#    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "#        \"bert-fold{}.h5\".format(fold+1), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "#        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]],\n",
    "              [start_ids[idxT,], end_ids[idxT,]], \n",
    "              epochs=epochs, batch_size=16, verbose=1, #callbacks=[sv],\n",
    "              validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "                               [start_ids[idxV,], end_ids[idxV,]]))\n",
    "    \n",
    "#    print('Loading model...')\n",
    "#    model.load_weights('bert-fold%i.h5'%(fold))\n",
    "#    shutil.copy2(r'bert-fold{}.h5'.format(fold+1), r'/content/gdrive/My Drive/')\n",
    "\n",
    "    print('Predicting out-of-fold answer span')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=1)\n",
    "      \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    jac_oof = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        true_answer = train.loc[k,'selected_text']\n",
    "        if (train.loc[k, 'sentiment'] == 'neutral'):\n",
    "            pred_answer = train.loc[k, 'text']\n",
    "            pred_answer = pred_answer.strip() \n",
    "        elif a>b: \n",
    "            encoding = tokenizer.encode(train.loc[k, 'sentiment'], train.loc[k,'text'])\n",
    "            pred_answer = tokenizer.decode(encoding.ids[a:-1]) \n",
    "            pred_answer = pred_answer.strip() \n",
    "        else:\n",
    "            encoding = tokenizer.encode(train.loc[k, 'sentiment'], train.loc[k,'text'])\n",
    "            pred_answer = tokenizer.decode(encoding.ids[a:b+1]) \n",
    "            pred_answer = pred_answer.strip() \n",
    "        jac_oof.append(jaccard(pred_answer, true_answer))\n",
    "    jaccard_scores.append(np.mean(jac_oof))\n",
    "    print('FOLD %i Jaccard ='%(fold+1),np.mean(jac_oof))\n",
    "\n",
    "    print('Predicting Test answer span')\n",
    "    preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test])\n",
    "    preds_start += preds[0]/kfold_n_splits\n",
    "    preds_end += preds[1]/kfold_n_splits\n",
    "  return {'pred_test_start': preds_start, 'pred_test_end': preds_end,\n",
    "          'jaccard_kfold': jaccard_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21984/21984 [==============================] - 393s 18ms/sample - loss: 2.0160 - activation_loss: 1.0067 - activation_1_loss: 1.0093 - val_loss: 1.7305 - val_activation_loss: 0.8770 - val_activation_1_loss: 0.8530\n",
      "Epoch 2/3\n",
      "21984/21984 [==============================] - 376s 17ms/sample - loss: 1.5454 - activation_loss: 0.7749 - activation_1_loss: 0.7705 - val_loss: 1.7654 - val_activation_loss: 0.8815 - val_activation_1_loss: 0.8833\n",
      "Epoch 3/3\n",
      "21984/21984 [==============================] - 376s 17ms/sample - loss: 1.1698 - activation_loss: 0.5910 - activation_1_loss: 0.5788 - val_loss: 2.1107 - val_activation_loss: 1.0430 - val_activation_1_loss: 1.0667\n",
      "Predicting out-of-fold answer span\n",
      "5497/5497 [==============================] - 33s 6ms/sample\n",
      "FOLD 1 Jaccard = 0.6675426273561148\n",
      "Predicting Test answer span\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21985/21985 [==============================] - 424s 19ms/sample - loss: 2.0674 - activation_loss: 1.0350 - activation_1_loss: 1.0349 - val_loss: 1.7503 - val_activation_loss: 0.8766 - val_activation_1_loss: 0.8736\n",
      "Epoch 2/3\n",
      "21985/21985 [==============================] - 403s 18ms/sample - loss: 1.5671 - activation_loss: 0.7837 - activation_1_loss: 0.7838 - val_loss: 1.7304 - val_activation_loss: 0.8744 - val_activation_1_loss: 0.8558\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 404s 18ms/sample - loss: 1.1935 - activation_loss: 0.5974 - activation_1_loss: 0.5953 - val_loss: 2.0139 - val_activation_loss: 1.0403 - val_activation_1_loss: 0.9729\n",
      "Predicting out-of-fold answer span\n",
      "5496/5496 [==============================] - 33s 6ms/sample\n",
      "FOLD 2 Jaccard = 0.6778968652449685\n",
      "Predicting Test answer span\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21985/21985 [==============================] - 424s 19ms/sample - loss: 2.0584 - activation_loss: 1.0331 - activation_1_loss: 1.0238 - val_loss: 1.7736 - val_activation_loss: 0.9003 - val_activation_1_loss: 0.8764\n",
      "Epoch 2/3\n",
      "21985/21985 [==============================] - 403s 18ms/sample - loss: 1.5559 - activation_loss: 0.7816 - activation_1_loss: 0.7740 - val_loss: 1.8070 - val_activation_loss: 0.8885 - val_activation_1_loss: 0.9209\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 403s 18ms/sample - loss: 1.1875 - activation_loss: 0.5991 - activation_1_loss: 0.5876 - val_loss: 1.8678 - val_activation_loss: 0.9324 - val_activation_1_loss: 0.9378\n",
      "Predicting out-of-fold answer span\n",
      "5496/5496 [==============================] - 32s 6ms/sample\n",
      "FOLD 3 Jaccard = 0.6650881710613895\n",
      "Predicting Test answer span\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21985/21985 [==============================] - 404s 18ms/sample - loss: 1.5473 - activation_loss: 0.7745 - activation_1_loss: 0.7719 - val_loss: 1.8046 - val_activation_loss: 0.8938 - val_activation_1_loss: 0.9107\n",
      "Epoch 3/3\n",
      "21985/21985 [==============================] - 404s 18ms/sample - loss: 1.1779 - activation_loss: 0.5948 - activation_1_loss: 0.5833 - val_loss: 1.9527 - val_activation_loss: 0.9662 - val_activation_1_loss: 0.9863\n",
      "Predicting out-of-fold answer span\n",
      "5496/5496 [==============================] - 32s 6ms/sample\n",
      "FOLD 4 Jaccard = 0.668526895356261\n",
      "Predicting Test answer span\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21985/21985 [==============================] - 424s 19ms/sample - loss: 2.0256 - activation_loss: 1.0158 - activation_1_loss: 1.0085 - val_loss: 1.7252 - val_activation_loss: 0.8476 - val_activation_1_loss: 0.8769\n",
      "Epoch 2/3\n",
      "11872/21985 [===============>..............] - ETA: 2:49 - loss: 1.1668 - activation_loss: 0.5917 - activation_1_loss: 0.5751"
     ]
    }
   ],
   "source": [
    "out = train_model(input_ids, attention_mask, token_type_ids, input_ids_test,attention_mask_test,token_type_ids_test,kfold_n_splits = 5, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_inf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a5cb3f708491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Jaccard (CV): {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_inf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jaccard_kfold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out_inf' is not defined"
     ]
    }
   ],
   "source": [
    "print('Mean Jaccard (CV): {}'.format(np.mean(out_inf['jaccard_kfold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  selected_text\n",
       "0  f87dea47db            NaN\n",
       "1  96d74cb729            NaN\n",
       "2  eee518ae67            NaN\n",
       "3  01082688c6            NaN\n",
       "4  33987a8ee5            NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "\n",
    "preds_start, preds_end = out['pred_test_start'], out['pred_test_end']\n",
    "for i in range(input_ids_test.shape[0]):\n",
    "    a = np.argmax(preds_start[i,])\n",
    "    b = np.argmax(preds_end[i,])\n",
    "    if (test.loc[i, 'sentiment'] == 'neutral'):\n",
    "        pred_answer = test.loc[i, 'text']\n",
    "        test.loc[i,'selected_text'] = pred_answer\n",
    "        pred_answer = pred_answer.strip() \n",
    "    elif a>b:\n",
    "        encoding = tokenizer.encode(test.loc[i, 'sentiment'], test.loc[i,'text'])\n",
    "        pred_answer = tokenizer.decode(encoding.ids[a:-1])  \n",
    "        test.loc[i,'selected_text'] = pred_answer\n",
    "        pred_answer = pred_answer.strip() \n",
    "    else:\n",
    "        encoding = tokenizer.encode(test.loc[i, 'sentiment'], test.loc[i,'text'])\n",
    "        pred_answer = tokenizer.decode(encoding.ids[a:b+1])  \n",
    "        test.loc[i,'selected_text'] = pred_answer\n",
    "        pred_answer = pred_answer.strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only *textID* and *selected_text* need to be saved in submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>shanghai is also really exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>such a shame!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy bday!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like it!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>726e501993</td>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td>positive</td>\n",
       "      <td>that ` s great!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>261932614e</td>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>hates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>afa11da83f</td>\n",
       "      <td>soooooo wish i could, but im in school and my...</td>\n",
       "      <td>negative</td>\n",
       "      <td>completely blocked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e64208b4ef</td>\n",
       "      <td>and within a short time of the last clue all ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>and within a short time of the last clue all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37bcad24ca</td>\n",
       "      <td>What did you get?  My day is alright.. haven`...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>What did you get?  My day is alright.. haven`...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24c92644a4</td>\n",
       "      <td>My bike was put on hold...should have known th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bummer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>43b390b336</td>\n",
       "      <td>I checked.  We didn`t win</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I checked.  We didn`t win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>69d6b5d93e</td>\n",
       "      <td>.. and you`re on twitter! Did the tavern bore...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.. and you`re on twitter! Did the tavern bore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5c1e0b61a1</td>\n",
       "      <td>I`m in VA for the weekend, my youngest son tur...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sad,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>504e45d9d9</td>\n",
       "      <td>Its coming out the socket  I feel like my phon...</td>\n",
       "      <td>negative</td>\n",
       "      <td>its coming out the socket i feel like my phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ae93ad52a0</td>\n",
       "      <td>So hot today =_=  don`t like it and i hate my ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>so hot today = _ = don ` t like it and i hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9fce30159a</td>\n",
       "      <td>Miss you</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d5195223</td>\n",
       "      <td>Cramps . . .</td>\n",
       "      <td>negative</td>\n",
       "      <td>cramps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>33f19050cf</td>\n",
       "      <td>you guys didn`t say hi or answer my questions...</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>f7718b3c23</td>\n",
       "      <td>I`m going into a spiritual stagnentation, its ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`m going into a spiritual stagnentation, its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9ef44428d0</td>\n",
       "      <td>Stupid storm. No river for us tonight</td>\n",
       "      <td>negative</td>\n",
       "      <td>stupid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>be634ebeb0</td>\n",
       "      <td>My dead grandpa pays more attention to me than...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my dead grandpa pays more attention to me than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3dcf4f7e13</td>\n",
       "      <td>... need retail therapy, bad. AHHH.....gimme m...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>f0ef04109b</td>\n",
       "      <td>about to go to sleep</td>\n",
       "      <td>neutral</td>\n",
       "      <td>about to go to sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8be365118e</td>\n",
       "      <td>you are lame  go make me breakfast!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>you are lame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>c50bdd4567</td>\n",
       "      <td>thats so cool</td>\n",
       "      <td>positive</td>\n",
       "      <td>thats so cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>334954f215</td>\n",
       "      <td>hey peoples, dont you just hate being grounded...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hey peoples, dont you just hate being grounded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b783916431</td>\n",
       "      <td>Huh, another ScarePoint coding Sunday</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Huh, another ScarePoint coding Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1fa8e6ad66</td>\n",
       "      <td>look who I found just for you  ---&gt;  http://t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>look who i found just for you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>be38b29042</td>\n",
       "      <td>No AC, the fan doesnt swing our way ... we are...</td>\n",
       "      <td>negative</td>\n",
       "      <td>no ac, the fan doesnt swing our way... we are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text sentiment  \\\n",
       "0   f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1   96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2   eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3   01082688c6                                        happy bday!  positive   \n",
       "4   33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "5   726e501993                    that`s great!! weee!! visitors!  positive   \n",
       "6   261932614e            I THINK EVERYONE HATES ME ON HERE   lol  negative   \n",
       "7   afa11da83f   soooooo wish i could, but im in school and my...  negative   \n",
       "8   e64208b4ef   and within a short time of the last clue all ...   neutral   \n",
       "9   37bcad24ca   What did you get?  My day is alright.. haven`...   neutral   \n",
       "10  24c92644a4  My bike was put on hold...should have known th...  negative   \n",
       "11  43b390b336                          I checked.  We didn`t win   neutral   \n",
       "12  69d6b5d93e   .. and you`re on twitter! Did the tavern bore...   neutral   \n",
       "13  5c1e0b61a1  I`m in VA for the weekend, my youngest son tur...  negative   \n",
       "14  504e45d9d9  Its coming out the socket  I feel like my phon...  negative   \n",
       "15  ae93ad52a0  So hot today =_=  don`t like it and i hate my ...  negative   \n",
       "16  9fce30159a                                           Miss you  negative   \n",
       "17  00d5195223                                       Cramps . . .  negative   \n",
       "18  33f19050cf   you guys didn`t say hi or answer my questions...  positive   \n",
       "19  f7718b3c23  I`m going into a spiritual stagnentation, its ...   neutral   \n",
       "20  9ef44428d0              Stupid storm. No river for us tonight  negative   \n",
       "21  be634ebeb0  My dead grandpa pays more attention to me than...  negative   \n",
       "22  3dcf4f7e13  ... need retail therapy, bad. AHHH.....gimme m...  negative   \n",
       "23  f0ef04109b                               about to go to sleep   neutral   \n",
       "24  8be365118e               you are lame  go make me breakfast!!  negative   \n",
       "25  c50bdd4567                                      thats so cool  positive   \n",
       "26  334954f215  hey peoples, dont you just hate being grounded...   neutral   \n",
       "27  b783916431              Huh, another ScarePoint coding Sunday   neutral   \n",
       "28  1fa8e6ad66   look who I found just for you  --->  http://t...  positive   \n",
       "29  be38b29042  No AC, the fan doesnt swing our way ... we are...  negative   \n",
       "\n",
       "                                        selected_text  \n",
       "0   Last session of the day  http://twitpic.com/67ezh  \n",
       "1                    shanghai is also really exciting  \n",
       "2                                       such a shame!  \n",
       "3                                         happy bday!  \n",
       "4                                         i like it!!  \n",
       "5                                     that ` s great!  \n",
       "6                                               hates  \n",
       "7                                  completely blocked  \n",
       "8    and within a short time of the last clue all ...  \n",
       "9    What did you get?  My day is alright.. haven`...  \n",
       "10                                             bummer  \n",
       "11                          I checked.  We didn`t win  \n",
       "12   .. and you`re on twitter! Did the tavern bore...  \n",
       "13                                               sad,  \n",
       "14  its coming out the socket i feel like my phone...  \n",
       "15  so hot today = _ = don ` t like it and i hate ...  \n",
       "16                                           miss you  \n",
       "17                                          cramps...  \n",
       "18                                               nice  \n",
       "19  I`m going into a spiritual stagnentation, its ...  \n",
       "20                                             stupid  \n",
       "21  my dead grandpa pays more attention to me than...  \n",
       "22                                               bad.  \n",
       "23                               about to go to sleep  \n",
       "24                                       you are lame  \n",
       "25                                      thats so cool  \n",
       "26  hey peoples, dont you just hate being grounded...  \n",
       "27              Huh, another ScarePoint coding Sunday  \n",
       "28                      look who i found just for you  \n",
       "29  no ac, the fan doesnt swing our way... we are ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['textID', 'selected_text']].to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
